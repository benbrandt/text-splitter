[package]
name = "text-splitter"
version = "0.4.0"
authors = ["Ben Brandt <benjamin.j.brandt@gmail.com>"]
edition = "2021"
description = "Split text into semantic chunks, up to a desired chunk size. Supports calculating length by characters and tokens (when used with large language models)."
repository = "https://github.com/benbrandt/text-splitter"
license = "MIT"
keywords = ["text", "split", "tokenizer", "nlp", "ai"]
categories = ["text-processing"]
exclude = ["/tests/snapshots/**", "/tests/inputs/**"]
rust-version = "1.60.0"

[package.metadata.docs.rs]
all-features = true
rustdoc-args = ["--cfg", "docsrs"]

# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html

[dependencies]
auto_enums = "0.8.0"
either = "1.8.1"
itertools = "0.10.5"
once_cell = "1.17.2"
regex = "1.8.3"
tiktoken-rs = { version = "0.4.2", optional = true }
tokenizers = { version = "0.13.3", optional = true }
unicode-segmentation = "1.10.1"

[dev-dependencies]
fake = "2.6.1"
insta = { version = "1.29.0", features = ["glob", "yaml"] }
more-asserts = "0.3.1"

[features]
tokenizers = ["dep:tokenizers"]
tiktoken-rs = ["dep:tiktoken-rs"]

# Tokenizers and indirect deps can cause slow runtime
[profile.dev.package."*"]
opt-level = 1

[profile.dev.package.insta]
opt-level = 3

[profile.dev.package.similar]
opt-level = 3
