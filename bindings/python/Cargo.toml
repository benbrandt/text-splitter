[package]
name = "semantic-text-splitter"
version = "0.2.3"
authors = ["Ben Brandt <benjamin.j.brandt@gmail.com>"]
edition = "2021"
description = "Split text into semantic chunks, up to a desired chunk size. Supports calculating length by characters and tokens (when used with large language models)."
repository = "https://github.com/benbrandt/text-splitter"
license = "MIT"
keywords = ["text", "split", "tokenizer", "nlp", "ai"]

# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html
[lib]
name = "semantic_text_splitter"
crate-type = ["cdylib"]

[dependencies]
pyo3 = { version = "0.19.2", features = ["abi3-py37"] }
text-splitter = { version = "0.4.4", features = ["tiktoken-rs", "tokenizers"] }
tiktoken-rs = "0.5.3"
tokenizers = { version = "0.14.0", default_features = false, features = [
    "onig",
] }
