[package]
name = "semantic-text-splitter"
version = "0.6.0"
authors = ["Ben Brandt <benjamin.j.brandt@gmail.com>"]
edition = "2021"
description = "Split text into semantic chunks, up to a desired chunk size. Supports calculating length by characters and tokens (when used with large language models)."
repository = "https://github.com/benbrandt/text-splitter"
license = "MIT"
keywords = ["text", "split", "tokenizer", "nlp", "ai"]

# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html
[lib]
name = "semantic_text_splitter"
crate-type = ["cdylib"]

[dependencies]
pyo3 = { version = "0.20.2", features = ["abi3-py38"] }
text-splitter = { path = "../..", features = ["tiktoken-rs", "tokenizers"] }
tiktoken-rs = "0.5.8"
tokenizers = { version = "0.15.0", default_features = false, features = [
    "onig",
] }
